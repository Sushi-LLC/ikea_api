namespace :products do
  desc "ÐžÐ±Ð½Ð¾Ð²Ð¸Ñ‚ÑŒ Ð²ÑÐµ Ð¿Ñ€Ð¾Ð´ÑƒÐºÑ‚Ñ‹ 'Ð¥Ð¸Ñ‚Ñ‹ Ð¿Ñ€Ð¾Ð´Ð°Ð¶' Ñ‡ÐµÑ€ÐµÐ· scrape.do"
  task update_bestsellers: :environment do
    puts "=== ÐžÐ‘ÐÐžÐ’Ð›Ð•ÐÐ˜Ð• ÐŸÐ ÐžÐ”Ð£ÐšÐ¢ÐžÐ’ 'Ð¥Ð˜Ð¢Ð« ÐŸÐ ÐžÐ”ÐÐ–' ==="
    puts ""
    
    # ÐÐ°Ñ…Ð¾Ð´Ð¸Ð¼ Ð²ÑÐµ Ð¿Ñ€Ð¾Ð´ÑƒÐºÑ‚Ñ‹ Ñ Ñ„Ð»Ð°Ð³Ð¾Ð¼ is_bestseller
    bestsellers = Product.where(is_bestseller: true)
    total_count = bestsellers.count
    
    puts "ÐÐ°Ð¹Ð´ÐµÐ½Ð¾ Ð¿Ñ€Ð¾Ð´ÑƒÐºÑ‚Ð¾Ð² 'Ð¥Ð¸Ñ‚Ñ‹ Ð¿Ñ€Ð¾Ð´Ð°Ð¶': #{total_count}"
    puts ""
    
    if total_count == 0
      puts "âš ï¸  ÐÐµÑ‚ Ð¿Ñ€Ð¾Ð´ÑƒÐºÑ‚Ð¾Ð² Ñ Ñ„Ð»Ð°Ð³Ð¾Ð¼ is_bestseller = true"
      puts "   Ð—Ð°Ð¿ÑƒÑÑ‚Ð¸Ñ‚Ðµ ÑÐ½Ð°Ñ‡Ð°Ð»Ð° ParseBestsellersJob Ð´Ð»Ñ ÑƒÑÑ‚Ð°Ð½Ð¾Ð²ÐºÐ¸ Ñ„Ð»Ð°Ð³Ð¾Ð²"
      exit
    end
    
    stats = {
      processed: 0,
      updated: 0,
      errors: 0,
      images_downloaded: 0
    }
    
    bestsellers.find_each.with_index do |product, index|
      stats[:processed] += 1
      
      puts "[#{stats[:processed]}/#{total_count}] ÐžÐ±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ° Ð¿Ñ€Ð¾Ð´ÑƒÐºÑ‚Ð°: #{product.name} (SKU: #{product.sku})"
      
      begin
        # ÐŸÑ€Ð¾Ð²ÐµÑ€ÑÐµÐ¼ Ð½Ð°Ð»Ð¸Ñ‡Ð¸Ðµ URL
        if product.url.blank?
          puts "  âš ï¸  ÐŸÑ€Ð¾Ð¿ÑƒÑ‰ÐµÐ½: Ð½ÐµÑ‚ URL"
          stats[:errors] += 1
          next
        end
        
        # Ð¤Ð¾Ñ€Ð¼Ð¸Ñ€ÑƒÐµÐ¼ Ð¿Ð¾Ð»Ð½Ñ‹Ð¹ URL
        product_url = product.url.start_with?('http') ? product.url : "https://www.ikea.com#{product.url}"
        puts "  ðŸ“„ URL: #{product_url}"
        
        # 1. Ð—Ð°Ð¿Ñ€Ð°ÑˆÐ¸Ð²Ð°ÐµÐ¼ ÑÑ‚Ñ€Ð°Ð½Ð¸Ñ†Ñƒ Ñ‡ÐµÑ€ÐµÐ· scrape.do Ð´Ð»Ñ Ð¿Ð¾Ð»ÑƒÑ‡ÐµÐ½Ð¸Ñ Ð¿Ð¾Ð»Ð½Ñ‹Ñ… Ð´Ð°Ð½Ð½Ñ‹Ñ…
        puts "  ðŸ”„ Ð—Ð°Ð¿Ñ€Ð¾Ñ ÑÑ‚Ñ€Ð°Ð½Ð¸Ñ†Ñ‹ Ñ‡ÐµÑ€ÐµÐ· scrape.do..."
        scrape_do_html = ScrapeDoHelper.fetch_via_scrape_do(product_url)
        
        if scrape_do_html && scrape_do_html.length > 1000
          puts "  âœ“ HTML Ð¿Ð¾Ð»ÑƒÑ‡ÐµÐ½ (#{scrape_do_html.length} ÑÐ¸Ð¼Ð²Ð¾Ð»Ð¾Ð²)"
          
          # ÐŸÐ°Ñ€ÑÐ¸Ð¼ HTML Ñ‡ÐµÑ€ÐµÐ· PlDetailsFetcher
          puts "  ðŸ”„ ÐŸÐ°Ñ€ÑÐ¸Ð½Ð³ HTML Ñ‡ÐµÑ€ÐµÐ· PlDetailsFetcher..."
          pl_details = PlDetailsFetcher.parse_html(scrape_do_html, product_url)
          
          if pl_details.present?
            puts "  âœ“ Ð”Ð°Ð½Ð½Ñ‹Ðµ Ð¸Ð·Ð²Ð»ÐµÑ‡ÐµÐ½Ñ‹: Ñ†ÐµÐ½Ð°=#{pl_details[:price]}, Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ð¹=#{Array(pl_details[:images] || []).length}"
            
            # ÐŸÐ¾Ð´Ð³Ð¾Ñ‚Ð°Ð²Ð»Ð¸Ð²Ð°ÐµÐ¼ Ð´Ð°Ð½Ð½Ñ‹Ðµ Ð´Ð»Ñ process_product
            puts "  ðŸ”„ ÐžÐ±Ð½Ð¾Ð²Ð»ÐµÐ½Ð¸Ðµ Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð¿Ñ€Ð¾Ð´ÑƒÐºÑ‚Ð°..."
            product_data = {
              'id' => product.sku,
              'sku' => product.sku,
              'itemNo' => product.item_no || pl_details[:sku],
              'name' => pl_details[:name] || product.name,
              'url' => product_url,
              'price' => pl_details[:price] || product.price,
              'images' => (Array(product.images || []) + Array(pl_details[:images] || [])).compact.uniq
            }
            
            # Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÐ¼ Ð¼ÐµÑ‚Ð¾Ð´ process_product Ð¸Ð· ParseProductsJob
            category = product.category || Category.first
            if category
              job = ParseProductsJob.new
              job.send(:process_product, product_data, category)
              product.reload
              puts "  âœ“ Ð”Ð°Ð½Ð½Ñ‹Ðµ Ð¾Ð±Ð½Ð¾Ð²Ð»ÐµÐ½Ñ‹"
              stats[:updated] += 1
            else
              puts "  âš ï¸  ÐŸÑ€Ð¾Ð¿ÑƒÑ‰ÐµÐ½: Ð½ÐµÑ‚ ÐºÐ°Ñ‚ÐµÐ³Ð¾Ñ€Ð¸Ð¸"
              stats[:errors] += 1
              next
            end
          else
            puts "  âš ï¸  ÐÐµ ÑƒÐ´Ð°Ð»Ð¾ÑÑŒ Ð¸Ð·Ð²Ð»ÐµÑ‡ÑŒ Ð´Ð°Ð½Ð½Ñ‹Ðµ Ð¸Ð· HTML"
            stats[:errors] += 1
            next
          end
        else
          puts "  âš ï¸  ÐÐµ ÑƒÐ´Ð°Ð»Ð¾ÑÑŒ Ð¿Ð¾Ð»ÑƒÑ‡Ð¸Ñ‚ÑŒ HTML Ñ‡ÐµÑ€ÐµÐ· scrape.do, Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÐ¼ Ð¾Ð±Ñ‹Ñ‡Ð½Ñ‹Ð¹ Ð¿Ð°Ñ€ÑÐ¸Ð½Ð³"
          # Fallback Ð½Ð° Ð¾Ð±Ñ‹Ñ‡Ð½Ñ‹Ð¹ Ð¿Ð°Ñ€ÑÐ¸Ð½Ð³
          pl_details = PlDetailsFetcher.fetch(product_url)
          if pl_details.present?
            product_data = {
              'id' => product.sku,
              'sku' => product.sku,
              'itemNo' => product.item_no,
              'name' => product.name,
              'url' => product_url,
              'price' => product.price || pl_details[:price],
              'images' => (product.images || []) + Array(pl_details[:images] || [])
            }
            category = product.category || Category.first
            if category
              ParseProductsJob.new.send(:process_product, product_data, category)
              product.reload
              puts "  âœ“ Ð”Ð°Ð½Ð½Ñ‹Ðµ Ð¾Ð±Ð½Ð¾Ð²Ð»ÐµÐ½Ñ‹ (Ð¾Ð±Ñ‹Ñ‡Ð½Ñ‹Ð¹ Ð¿Ð°Ñ€ÑÐ¸Ð½Ð³)"
              stats[:updated] += 1
            end
          end
        end
        
        # 2. Ð—Ð°Ð³Ñ€ÑƒÐ¶Ð°ÐµÐ¼ Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ñ
        if product.images.present? || product.remote_images.present?
          image_urls = Array(product.images || product.remote_images || [])
          if image_urls.any?
            puts "  ðŸ–¼ï¸  Ð—Ð°Ð³Ñ€ÑƒÐ·ÐºÐ° #{image_urls.length} Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ð¹..."
            downloaded = ImageDownloader.download_product_images(product, image_urls)
            stats[:images_downloaded] += downloaded.length
            puts "  âœ“ Ð—Ð°Ð³Ñ€ÑƒÐ¶ÐµÐ½Ð¾ Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ð¹: #{downloaded.length}"
          end
        end
        
        puts "  âœ… ÐŸÑ€Ð¾Ð´ÑƒÐºÑ‚ Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚Ð°Ð½ ÑƒÑÐ¿ÐµÑˆÐ½Ð¾"
        puts ""
        
      rescue => e
        puts "  âŒ ÐžÑˆÐ¸Ð±ÐºÐ°: #{e.message}"
        puts "     #{e.backtrace.first(3).join("\n     ")}"
        stats[:errors] += 1
        puts ""
      end
    end
    
    puts ""
    puts "=== Ð˜Ð¢ÐžÐ“Ð˜ ==="
    puts "ÐžÐ±Ñ€Ð°Ð±Ð¾Ñ‚Ð°Ð½Ð¾: #{stats[:processed]}"
    puts "ÐžÐ±Ð½Ð¾Ð²Ð»ÐµÐ½Ð¾: #{stats[:updated]}"
    puts "ÐžÑˆÐ¸Ð±Ð¾Ðº: #{stats[:errors]}"
    puts "Ð˜Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ð¹ Ð·Ð°Ð³Ñ€ÑƒÐ¶ÐµÐ½Ð¾: #{stats[:images_downloaded]}"
    puts ""
  end
end

# Ð’ÑÐ¿Ð¾Ð¼Ð¾Ð³Ð°Ñ‚ÐµÐ»ÑŒÐ½Ñ‹Ð¹ Ð¼Ð¾Ð´ÑƒÐ»ÑŒ Ð´Ð»Ñ Ñ€Ð°Ð±Ð¾Ñ‚Ñ‹ Ñ scrape.do
module ScrapeDoHelper
  def self.fetch_via_scrape_do(url)
    api_token = ENV.fetch('SCRAPE_DO_API_TOKEN', '752d361f2e444064955c30f0dd3b93b896726e4944e')
    api_url = "https://api.scrape.do/"
    
    uri = URI.parse(api_url)
    http = Net::HTTP.new(uri.host, uri.port)
    http.use_ssl = true
    http.read_timeout = 60
    http.open_timeout = 30
    
    params = {
      'token' => api_token,
      'url' => url,
      'format' => 'html',
      'render' => 'true',
      'wait' => '5000'
    }
    
    request_uri = "#{uri.path}?#{URI.encode_www_form(params)}"
    request = Net::HTTP::Get.new(request_uri)
    request['User-Agent'] = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
    request['Accept'] = 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8'
    
    response = http.request(request)
    
    if response.is_a?(Net::HTTPSuccess)
      response.body
    else
      Rails.logger.error "Scrape.do API error: HTTP #{response.code} - #{response.message}"
      nil
    end
  rescue => e
    Rails.logger.error "Scrape.do API exception: #{e.class} - #{e.message}"
    nil
  end
end

